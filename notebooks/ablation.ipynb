{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f140f57f",
   "metadata": {
    "lines_to_next_cell": 0,
    "id": "f140f57f"
   },
   "source": "# SEG-PEFT - Targeted Ablation on 2D Segmentation\n\nThis notebook performs a comprehensive ablation study comparing Full Fine-Tuning (FFT) against LoRA with varying rank and alpha configurations on the Kvasir-SEG dataset.\n\n## Objective\nEvaluate how LoRA hyperparameters (rank r and scaling factor α) affect segmentation performance compared to FFT baseline.\n\n## Experimental Setup\n- **Dataset**: Kvasir-SEG (gastrointestinal polyp segmentation)\n- **Model**: SegFormer-B0 pretrained on ADE20K\n- **Training**: 50 epochs, learning rate 5e-4, dropout 0.0\n- **Evaluation**: Epoch-based metrics (Mean Dice, Mean IoU, Mean Accuracy)\n\n## Experiments\n1. **FFT Baseline**: Full model fine-tuning\n2. **LoRA Ablation**: 15 configurations with rank r ∈ {4, 8, 16, 32, 64} and α/r ratios ∈ {1, 2, 4}"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52aba103",
   "metadata": {
    "lines_to_next_cell": 0,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52aba103",
    "outputId": "dcd40ea8-6485-42ce-d0ed-e3fd9146868e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'SEG-PEFT'...\n",
      "remote: Enumerating objects: 162, done.\u001B[K\n",
      "remote: Counting objects: 100% (162/162), done.\u001B[K\n",
      "remote: Compressing objects: 100% (89/89), done.\u001B[K\n",
      "remote: Total 162 (delta 75), reused 128 (delta 45), pack-reused 0 (from 0)\u001B[K\n",
      "Receiving objects: 100% (162/162), 211.49 KiB | 15.11 MiB/s, done.\n",
      "Resolving deltas: 100% (75/75), done.\n",
      "/content/SEG-PEFT\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m7.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rossoc/SEG-PEFT\n",
    "%cd SEG-PEFT\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b76fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "767fe80f162642b9abb6696cead4a429",
      "b3db02e3455643caa89d8bd44d4a9943",
      "339d3bb9f46e4f12b9df5d936ec81a51",
      "76b55ce72d0a4d55b5f3c1e30413d3c0",
      "444416047a0d47828cb4c74b080eb11f",
      "5d944918cd464e91b90a16863259425a",
      "9cccf7ee871442d68b380832c7c3d6db",
      "0d280cbbf98748a68b8c12c37bf7e409",
      "f50b497930a74c5f9e61cb7f0cb28c30",
      "93d15dcf8cc74bd0ac333cec77483952",
      "23b37aecbc1248e0908c6b8f2a07bd38"
     ]
    },
    "id": "03b76fec",
    "outputId": "c69ec5b6-c303-42f5-c028-1ac4c01c32a6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "767fe80f162642b9abb6696cead4a429"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from src.segpeft import kvasir_dataset, compute_metrics, segformer, set_seed, Metrics\n",
    "import time\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "id": "xy8r6xbu3r9",
   "source": "# Colab utility: Download experiment results\n# After each experiment, results are zipped and downloaded automatically\nimport shutil\nfrom google.colab import files\n\ndef download_results(save_dir):\n    \"\"\"Zip and download experiment results folder\"\"\"\n    output_path = f\"./outputs/{save_dir}\"\n    zip_name = f\"{save_dir}_results\"\n    \n    # Create zip file\n    shutil.make_archive(zip_name, 'zip', output_path)\n    \n    # Download the zip file\n    files.download(f\"{zip_name}.zip\")\n    print(f\"Downloaded {zip_name}.zip\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5f42381d",
   "metadata": {
    "id": "5f42381d"
   },
   "source": "## Dataset Setup\n\nDownload and prepare the Kvasir-SEG dataset containing 1000 polyp images with corresponding segmentation masks. The dataset is split into 80% training and 20% validation.\n\nDataset: [Kvasir-SEG](https://datasets.simula.no/kvasir-seg/)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea54000",
   "metadata": {
    "lines_to_next_cell": 2,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ea54000",
    "outputId": "72ada3df-79ef-4133-f004-87c6b69a797b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2025-11-05 19:26:48--  https://datasets.simula.no/downloads/kvasir-seg.zip\n",
      "Resolving datasets.simula.no (datasets.simula.no)... 128.39.36.14\n",
      "Connecting to datasets.simula.no (datasets.simula.no)|128.39.36.14|:443... connected.\n",
      "WARNING: cannot verify datasets.simula.no's certificate, issued by ‘CN=Sectigo Public Server Authentication CA DV R36,O=Sectigo Limited,C=GB’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 46227172 (44M) [application/zip]\n",
      "Saving to: ‘kvasir-seg.zip’\n",
      "\n",
      "kvasir-seg.zip      100%[===================>]  44.08M  12.4MB/s    in 3.6s    \n",
      "\n",
      "2025-11-05 19:26:53 (12.4 MB/s) - ‘kvasir-seg.zip’ saved [46227172/46227172]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"data\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "!wget --no-check-certificate https://datasets.simula.no/downloads/kvasir-seg.zip -O kvasir-seg.zip\n",
    "\n",
    "with zipfile.ZipFile(\"kvasir-seg.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324d223",
   "metadata": {
    "lines_to_next_cell": 2,
    "id": "6324d223"
   },
   "source": "## Baseline: Full Fine-Tuning (FFT)\n\nTrain the complete SegFormer model with all parameters trainable. This serves as the performance upper bound for comparison with LoRA experiments.\n\n**Key characteristics:**\n- All 3.7M parameters are trainable\n- Higher computational cost and memory requirements\n- Evaluation and logging performed every epoch"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735eb34a",
   "metadata": {
    "id": "735eb34a"
   },
   "outputs": [],
   "source": "batch_size = 64\ngradient_accumulation_steps = 4\nuse_bf16 = True\ndataloader_num_workers = 8\n\ndef train_segformer_fft(epochs, lr, save_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    test_size = 0.2\n    model, model_name, _ = segformer()\n    train_dataset, test_dataset = kvasir_dataset(model_name, test_size)\n    N = len(train_dataset)\n\n    training_args = TrainingArguments(\n        output_dir=\"./outputs/\" + save_dir,\n        num_train_epochs=epochs,\n\n        # A100 Optimization: Larger batch sizes\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size * 2,  # Can use larger batch for eval\n\n        # A100 Optimization: Gradient accumulation for effective larger batches\n        gradient_accumulation_steps=gradient_accumulation_steps,\n\n        # A100 Optimization: Mixed precision with bfloat16 (A100's specialty)\n        bf16=use_bf16 and torch.cuda.is_available(),\n        bf16_full_eval=use_bf16 and torch.cuda.is_available(),\n\n        # A100 Optimization: Efficient data loading\n        dataloader_num_workers=dataloader_num_workers,\n        dataloader_pin_memory=True,\n        dataloader_prefetch_factor=2,\n\n        # Training settings - EPOCH-BASED\n        learning_rate=lr,\n        save_total_limit=2,\n        prediction_loss_only=False,\n        remove_unused_columns=True,\n        push_to_hub=False,\n        report_to=\"none\",\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_strategy=\"epoch\",  # Log every epoch\n        load_best_model_at_end=True,\n        logging_dir=f\"./outputs/{save_dir}/logs\",\n\n        # Performance optimization\n        optim=\"adamw_torch_fused\" if torch.cuda.is_available() else \"adamw_torch\",\n\n        # Better learning rate schedule\n        warmup_ratio=0.1,\n        lr_scheduler_type=\"cosine\",\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        compute_metrics=compute_metrics,  # type: ignore\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n    )\n\n    print(\"Starting training...\")\n    start_time = time.time()\n    trainer.train()\n    end_time = time.time() - start_time\n\n    all_metrics = {\n        \"training_history\": trainer.state.log_history,\n        \"final_evaluation\": trainer.evaluate(),\n        \"training_time\": end_time,\n    }\n\n    with open(f\"./outputs/{save_dir}/all_metrics.json\", \"w\") as f:\n        yaml.dump(all_metrics, f, indent=2)\n\n    df = pd.DataFrame(trainer.state.log_history)\n    df.to_csv(f\"./outputs/{save_dir}/training_history.csv\", index=False)\n    trainer.save_model(f\"./outputs/{save_dir}/final\")\n\n    metrics = Metrics(f\"./outputs/{save_dir}/\")\n    metrics.plot_curves(trainer.state.log_history)\n    return trainer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301f958",
   "metadata": {
    "lines_to_next_cell": 2,
    "id": "c301f958"
   },
   "outputs": [],
   "source": "# FFT Configuration\nepochs = 50\nlearning_rate = 5e-4\nsave_dir = \"segformer_fft_baseline\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e582452b",
   "metadata": {
    "lines_to_next_cell": 2,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e582452b",
    "outputId": "49376e60-c8b7-4d2e-c37d-b05eebf17b26"
   },
   "outputs": [],
   "source": "# Train FFT baseline\nfft_trainer = train_segformer_fft(epochs, learning_rate, save_dir)"
  },
  {
   "cell_type": "code",
   "id": "rfdiebw93gf",
   "source": "# Download FFT results\ndownload_results(save_dir)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cffa349c",
   "metadata": {
    "lines_to_next_cell": 2,
    "id": "cffa349c"
   },
   "source": [
    "## Train\n",
    "[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer) with\n",
    "LoRA.\n",
    "Namely, we use [PEFT](https://github.com/huggingface/peft) to implmenent LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56bda1d",
   "metadata": {
    "id": "e56bda1d"
   },
   "outputs": [],
   "source": "def train_segformer_lora(epochs, lr, r, lora_alpha, lora_dropout, save_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    test_size = 0.2\n    model, model_name, modules = segformer()\n\n    peft_config = LoraConfig(\n        r=r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        target_modules=modules,\n    )\n\n    model = get_peft_model(model, peft_config)\n\n    model.print_trainable_parameters()\n\n    train_dataset, test_dataset = kvasir_dataset(model_name, test_size)\n    N = len(train_dataset)\n\n    training_args = TrainingArguments(\n        output_dir=\"./outputs/\" + save_dir,\n        num_train_epochs=epochs,\n\n        # A100 Optimization: Larger batch sizes\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size * 2,  # Can use larger batch for eval\n\n        # A100 Optimization: Gradient accumulation for effective larger batches\n        gradient_accumulation_steps=gradient_accumulation_steps,\n\n        # A100 Optimization: Mixed precision with bfloat16 (A100's specialty)\n        bf16=use_bf16 and torch.cuda.is_available(),\n        bf16_full_eval=use_bf16 and torch.cuda.is_available(),\n\n        # A100 Optimization: Efficient data loading\n        dataloader_num_workers=dataloader_num_workers,\n        dataloader_pin_memory=True,\n        dataloader_prefetch_factor=2,\n\n        # Training settings - EPOCH-BASED\n        learning_rate=lr,\n        save_total_limit=2,\n        prediction_loss_only=False,\n        remove_unused_columns=True,\n        push_to_hub=False,\n        report_to=\"none\",\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_strategy=\"epoch\",  # Log every epoch\n        load_best_model_at_end=True,\n        logging_dir=f\"./outputs/{save_dir}/logs\",\n\n        # Performance optimization\n        optim=\"adamw_torch_fused\" if torch.cuda.is_available() else \"adamw_torch\",\n\n        # Better learning rate schedule\n        warmup_ratio=0.1,\n        lr_scheduler_type=\"cosine\",\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        compute_metrics=compute_metrics,  # type: ignore\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n    )\n\n    print(\"Starting training...\")\n    start_time = time.time()\n    trainer.train()\n    end_time = time.time() - start_time\n\n    all_metrics = {\n        \"training_history\": trainer.state.log_history,\n        \"final_evaluation\": trainer.evaluate(),\n        \"training_time\": end_time,\n    }\n\n    with open(f\"./outputs/{save_dir}/all_metrics.json\", \"w\") as f:\n        yaml.dump(all_metrics, f, indent=2)\n\n    df = pd.DataFrame(trainer.state.log_history)\n    df.to_csv(f\"./outputs/{save_dir}/training_history.csv\", index=False)\n    trainer.save_model(f\"./outputs/{save_dir}/final\")\n\n    metrics = Metrics(f\"./outputs/{save_dir}/\")\n    metrics.plot_curves(trainer.state.log_history)\n    return trainer"
  },
  {
   "cell_type": "markdown",
   "id": "3c6d6023",
   "metadata": {
    "id": "3c6d6023"
   },
   "source": "## LoRA Ablation Study\n\nParameter-efficient fine-tuning using Low-Rank Adaptation (LoRA). Only low-rank matrices are trained while the base model remains frozen, drastically reducing trainable parameters (~1.7% of full model).\n\n**Fixed hyperparameters across all experiments:**\n- Epochs: 50\n- Learning rate: 5e-4\n- Dropout: 0.0\n- Evaluation: Every epoch\n\n**Variable hyperparameters:**\n- Rank (r): Controls capacity of low-rank adaptation matrices\n- Alpha (α): Scaling factor for LoRA updates\n\nEach experiment follows this pattern: higher rank = more parameters but potentially better performance.\n\n---\n\n### Experiment 1: r=4, α ∈ {4, 8, 16}\n\nMinimal parameter overhead (~0.17% trainable). Tests different α/r scaling ratios (1, 2, 4) with the smallest rank."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14210062",
   "metadata": {
    "lines_to_next_cell": 0,
    "id": "14210062",
    "outputId": "a5d02028-af7e-42ed-bc54-0b494a6133d6"
   },
   "outputs": [],
   "source": "# r=4, alpha=4\nepochs = 50\nlearning_rate = 5e-4\nrank = 4\nlora_alpha = 4\nlora_dropout = 0.0\nsave_dir = \"lora_r4_alpha4\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r4_a4 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)"
  },
  {
   "cell_type": "code",
   "id": "jr10e6ku8a",
   "source": "download_results(\"lora_r4_alpha4\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e96f2d",
   "metadata": {
    "id": "35e96f2d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ye6lQ54BTSil"
   },
   "outputs": [],
   "source": "# r=4, alpha=8\nepochs = 50\nlearning_rate = 5e-4\nrank = 4\nlora_alpha = 8\nlora_dropout = 0.0\nsave_dir = \"lora_r4_alpha8\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r4_a8 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "id": "ye6lQ54BTSil"
  },
  {
   "cell_type": "code",
   "id": "6suawahv2wa",
   "source": "download_results(\"lora_r4_alpha8\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "outputId": "a5d02028-af7e-42ed-bc54-0b494a6133d6",
    "id": "vXnCo1TtVW4a"
   },
   "outputs": [],
   "source": "# r=4, alpha=16\nepochs = 50\nlearning_rate = 5e-4\nrank = 4\nlora_alpha = 16\nlora_dropout = 0.0\nsave_dir = \"lora_r4_alpha16\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r4_a16 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "id": "vXnCo1TtVW4a"
  },
  {
   "cell_type": "code",
   "id": "9nxtu2ojkhe",
   "source": "download_results(\"lora_r4_alpha16\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "outputId": "a5d02028-af7e-42ed-bc54-0b494a6133d6",
    "id": "HAOuNw_-VX6g"
   },
   "source": "### Experiment 2: r=8, α ∈ {8, 16, 32}\n\nDouble the rank capacity of Experiment 1. Evaluates if increased rank improves segmentation quality with the same α/r ratios.",
   "id": "HAOuNw_-VX6g"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbmuPCv5TTcP"
   },
   "outputs": [],
   "source": "# r=8, alpha=8\nepochs = 50\nlearning_rate = 5e-4\nrank = 8\nlora_alpha = 8\nlora_dropout = 0.0\nsave_dir = \"lora_r8_alpha8\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r8_a8 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "id": "kbmuPCv5TTcP"
  },
  {
   "cell_type": "code",
   "id": "ooj6nnxvoz",
   "source": "download_results(\"lora_r8_alpha8\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0RnLkZtTTmx"
   },
   "outputs": [],
   "source": "# r=8, alpha=16\nepochs = 50\nlearning_rate = 5e-4\nrank = 8\nlora_alpha = 16\nlora_dropout = 0.0\nsave_dir = \"lora_r8_alpha16\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r8_a16 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "id": "B0RnLkZtTTmx"
  },
  {
   "cell_type": "code",
   "id": "59a0f9ef37v",
   "source": "download_results(\"lora_r8_alpha16\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIrY_rueT1WD"
   },
   "outputs": [],
   "source": "# r=8, alpha=32\nepochs = 50\nlearning_rate = 5e-4\nrank = 8\nlora_alpha = 32\nlora_dropout = 0.0\nsave_dir = \"lora_r8_alpha32\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r8_a32 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "id": "zIrY_rueT1WD"
  },
  {
   "cell_type": "code",
   "id": "qdx86fl56gr",
   "source": "download_results(\"lora_r8_alpha32\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwt0r5MmT1oY"
   },
   "source": "### Experiment 3: r=16, α ∈ {16, 32, 64}\n\nMedium rank configuration. Explores the performance-parameter tradeoff in the mid-range.",
   "id": "jwt0r5MmT1oY"
  },
  {
   "cell_type": "code",
   "id": "cvdsi91nn3s",
   "source": "# r=16, alpha=16\nepochs = 50\nlearning_rate = 5e-4\nrank = 16\nlora_alpha = 16\nlora_dropout = 0.0\nsave_dir = \"lora_r16_alpha16\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r16_a16 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f1zavcl8ww8",
   "source": "download_results(\"lora_r16_alpha16\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jz17cfehpp",
   "source": "# r=16, alpha=32\nepochs = 50\nlearning_rate = 5e-4\nrank = 16\nlora_alpha = 32\nlora_dropout = 0.0\nsave_dir = \"lora_r16_alpha32\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r16_a32 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "50mrhdil0ep",
   "source": "download_results(\"lora_r16_alpha32\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2zgsxrstj0g",
   "source": "# r=16, alpha=64\nepochs = 50\nlearning_rate = 5e-4\nrank = 16\nlora_alpha = 64\nlora_dropout = 0.0\nsave_dir = \"lora_r16_alpha64\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r16_a64 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qscqi4d9xib",
   "source": "download_results(\"lora_r16_alpha64\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "di4s9xs51hc",
   "source": "### Experiment 4: r=32, α ∈ {32, 64, 128}\n\nHigh rank configuration with increased model capacity. Tests if higher rank approaches FFT performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "sdpvfpxskh",
   "source": "# r=32, alpha=32\nepochs = 50\nlearning_rate = 5e-4\nrank = 32\nlora_alpha = 32\nlora_dropout = 0.0\nsave_dir = \"lora_r32_alpha32\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r32_a32 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "xvfvw5dpz6f",
   "source": "download_results(\"lora_r32_alpha32\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "o5l0h7ksvid",
   "source": "# r=32, alpha=64\nepochs = 50\nlearning_rate = 5e-4\nrank = 32\nlora_alpha = 64\nlora_dropout = 0.0\nsave_dir = \"lora_r32_alpha64\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r32_a64 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gejr1k5paor",
   "source": "download_results(\"lora_r32_alpha64\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jpn3l6g5tl8",
   "source": "# r=32, alpha=128\nepochs = 50\nlearning_rate = 5e-4\nrank = 32\nlora_alpha = 128\nlora_dropout = 0.0\nsave_dir = \"lora_r32_alpha128\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r32_a128 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "tbj392f4j8",
   "source": "download_results(\"lora_r32_alpha128\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "t3qknt6vt1h",
   "source": "### Experiment 5: r=64, α ∈ {64, 128, 256}\n\nMaximum rank configuration with highest parameter count among LoRA experiments. Evaluates the upper limit of LoRA's expressiveness.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "un7v7wif3xe",
   "source": "# r=64, alpha=64\nepochs = 50\nlearning_rate = 5e-4\nrank = 64\nlora_alpha = 64\nlora_dropout = 0.0\nsave_dir = \"lora_r64_alpha64\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r64_a64 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qiozqjtozw",
   "source": "download_results(\"lora_r64_alpha64\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0ya0xq7q34j",
   "source": "# r=64, alpha=128\nepochs = 50\nlearning_rate = 5e-4\nrank = 64\nlora_alpha = 128\nlora_dropout = 0.0\nsave_dir = \"lora_r64_alpha128\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r64_a128 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ovl1ogu5nb",
   "source": "download_results(\"lora_r64_alpha128\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "zh87g4qp2pn",
   "source": "# r=64, alpha=256\nepochs = 50\nlearning_rate = 5e-4\nrank = 64\nlora_alpha = 256\nlora_dropout = 0.0\nsave_dir = \"lora_r64_alpha256\"\n\nprint(f\"Training LoRA with r={rank}, alpha={lora_alpha}\")\ntrainer_r64_a256 = train_segformer_lora(epochs, learning_rate, rank, lora_alpha, lora_dropout, save_dir)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2a6nn0t9z5c",
   "source": "download_results(\"lora_r64_alpha256\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "767fe80f162642b9abb6696cead4a429": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b3db02e3455643caa89d8bd44d4a9943",
       "IPY_MODEL_339d3bb9f46e4f12b9df5d936ec81a51",
       "IPY_MODEL_76b55ce72d0a4d55b5f3c1e30413d3c0"
      ],
      "layout": "IPY_MODEL_444416047a0d47828cb4c74b080eb11f"
     }
    },
    "b3db02e3455643caa89d8bd44d4a9943": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d944918cd464e91b90a16863259425a",
      "placeholder": "​",
      "style": "IPY_MODEL_9cccf7ee871442d68b380832c7c3d6db",
      "value": "Downloading builder script: "
     }
    },
    "339d3bb9f46e4f12b9df5d936ec81a51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d280cbbf98748a68b8c12c37bf7e409",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f50b497930a74c5f9e61cb7f0cb28c30",
      "value": 1
     }
    },
    "76b55ce72d0a4d55b5f3c1e30413d3c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93d15dcf8cc74bd0ac333cec77483952",
      "placeholder": "​",
      "style": "IPY_MODEL_23b37aecbc1248e0908c6b8f2a07bd38",
      "value": " 12.9k/? [00:00&lt;00:00, 1.39MB/s]"
     }
    },
    "444416047a0d47828cb4c74b080eb11f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d944918cd464e91b90a16863259425a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cccf7ee871442d68b380832c7c3d6db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d280cbbf98748a68b8c12c37bf7e409": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "f50b497930a74c5f9e61cb7f0cb28c30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "93d15dcf8cc74bd0ac333cec77483952": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23b37aecbc1248e0908c6b8f2a07bd38": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
